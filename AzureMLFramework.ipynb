{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Service Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialzize Workspace\n",
    "\n",
    "* Import base Azure ML packages\n",
    "* Check the SDK version\n",
    "* Connect to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.65\n",
      "azureml-demo\teastus2\tjcantrell-rg1\teastus2\n"
     ]
    }
   ],
   "source": [
    "# base packages to work with AMLS\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "\n",
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name your experiment here.\n",
    "experiment_name = 'framework'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a directory for the Training script and any custom Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob azuremldemo7980688565 azureml-blobstore-12aea559-641a-4a9b-919d-04ce0cc6ddd6\n"
     ]
    }
   ],
   "source": [
    "# Directory to write training script.\n",
    "# Code Directory\n",
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"AzureMLFramework\")\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "#Upload Data\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "#ds.upload(src_dir=data_folder, target_path='mnist', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create or Attach an existing compute resource\n",
    "\n",
    "I've added two sets of code to create the Compute Cluster. The first cell is a simple version that uses defaults to create the cluster. The second cell is an examle of a more configurable version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# Compute cluster creation.\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"AzureMLFramework\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compute cluster creation #2.\n",
    "# This section is a more configurable version of the cell above.\n",
    "# Use either the above cell or this one, but not both.\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"AzureMLFramework\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines how to get the environment set up. This section MUST be included and filled out. Please be aware that there are two sections. Conda dependencies and PIP dependencies. Proper identification of where the packages are installed from is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/azmnt/code/Users/jcantrell/AzureMLFramework/fwrk.yml'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "fwrk = Environment(\"fwrk\")\n",
    "\n",
    "fwrk.docker.enabled = True\n",
    "fwrk.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn',\n",
    "                                                                          'pandas',\n",
    "                                                                          'numpy',\n",
    "                                                                          'seaborn',\n",
    "                                                                          'category_encoders',\n",
    "                                                                          'lightgbm',\n",
    "                                                                          'papermill'\n",
    "                                                                         ])\n",
    "fwrk.python.conda_dependencies.add_pip_package(\"inference-schema[numpy-support]\")\n",
    "fwrk.python.conda_dependencies.save_to_file(\".\", script_folder + \"/fwrk.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Predictive Model\n",
    "\n",
    "#### Create Training Script\n",
    "\n",
    "This section creates a training script to be used by the Experiment to build the Machine Learning Model. The output is a pickle file that is used to create a web service for making predictions using the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/azmnt/code/Users/jcantrell/AzureMLFramework/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# height, width, shoe size\n",
    "X = [[181, 80, 44], [177, 70, 43], [160, 60, 38], [154, 54, 37], [166, 65, 40], [190, 90, 47], [175, 64, 39],\n",
    "     [177, 70, 40], [159, 55, 37], [171, 75, 42], [181, 85, 43]]\n",
    "\n",
    "Y = ['male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'female', 'male', 'male']\n",
    "\n",
    "clf = SVC()\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "print('Predicted value:', clf.predict([[190, 70, 43]]))\n",
    "print('Accuracy', clf.score(X,Y))\n",
    "\n",
    "print('Export the model to model.pkl')\n",
    "f = open('fwrk.pkl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "print('Import the model from model.pkl')\n",
    "f2 = open('fwrk.pkl', 'rb')\n",
    "clf2 = pickle.load(f2)\n",
    "\n",
    "X_new = [[154, 54, 35]]\n",
    "print('New Sample:', X_new)\n",
    "print('Predicted class:', clf2.predict(X_new))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=clf, filename='outputs/fwrk.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the Training Job to the Compute Cluster\n",
    "\n",
    "Run the experiment by submitting the estimator object. And you can navigate to Azure portal to monitor the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>framework</td><td>framework_1570576914_938b648c</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/09f45657-2113-4b94-8f6f-cca0449e7fc3/resourceGroups/jcantrell-rg1/providers/Microsoft.MachineLearningServices/workspaces/azureml-demo/experiments/framework/runs/framework_1570576914_938b648c\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: framework,\n",
       "Id: framework_1570576914_938b648c,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder, script='train.py')\n",
    "\n",
    "# Set compute target to the one created in previous step\n",
    "src.run_config.target = cpu_cluster.name\n",
    "\n",
    "# Set environment\n",
    "src.run_config.environment = fwrk\n",
    " \n",
    "run = exp.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training happens in the background. You can use `wait_for_completion` to block and wait until the model has completed training before running more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: framework_1570576914_938b648c\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/09f45657-2113-4b94-8f6f-cca0449e7fc3/resourceGroups/jcantrell-rg1/providers/Microsoft.MachineLearningServices/workspaces/azureml-demo/experiments/framework/runs/framework_1570576914_938b648c\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2019-10-08T23:24:49Z Successfully mounted a/an Azure File Shares at /mnt/batch/tasks/shared/LS_root/jobs/azureml-demo/azureml/framework_1570576914_938b648c/mounts/workspacefilestore\n",
      "2019-10-08T23:24:50Z Mounted //azuremldemo7980688565.file.core.windows.net/azureml-filestore-12aea559-641a-4a9b-919d-04ce0cc6ddd6 at /mnt/batch/tasks/shared/LS_root/jobs/azureml-demo/azureml/framework_1570576914_938b648c/mounts/workspacefilestore\n",
      "2019-10-08T23:24:50Z No blob file systems configured\n",
      "2019-10-08T23:24:50Z No unmanaged file systems configured\n",
      "2019-10-08T23:24:50Z Starting output-watcher...\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_ea4f16c22b1c1824315a89a744e988cb\n",
      "16c48d79e9cc: Pulling fs layer\n",
      "3c654ad3ed7d: Pulling fs layer\n",
      "6276f4f9c29d: Pulling fs layer\n",
      "a4bd43ad48ce: Pulling fs layer\n",
      "f190bad819d1: Pulling fs layer\n",
      "07f7c6b6ffe6: Pulling fs layer\n",
      "9bd8013aa86d: Pulling fs layer\n",
      "28efa186d852: Pulling fs layer\n",
      "8adfa74289f9: Pulling fs layer\n",
      "6865d2339eef: Pulling fs layer\n",
      "2a7bad8c8573: Pulling fs layer\n",
      "5fcc4cd817ce: Pulling fs layer\n",
      "2ebe4d9e0c47: Pulling fs layer\n",
      "0776e061f5e5: Pulling fs layer\n",
      "fd548bf41707: Pulling fs layer\n",
      "5db91f0f2b44: Pulling fs layer\n",
      "f7598aa00566: Pulling fs layer\n",
      "6865d2339eef: Waiting\n",
      "2a7bad8c8573: Waiting\n",
      "5fcc4cd817ce: Waiting\n",
      "2ebe4d9e0c47: Waiting\n",
      "0776e061f5e5: Waiting\n",
      "a4bd43ad48ce: Waiting\n",
      "fd548bf41707: Waiting\n",
      "5db91f0f2b44: Waiting\n",
      "f7598aa00566: Waiting\n",
      "f190bad819d1: Waiting\n",
      "9bd8013aa86d: Waiting\n",
      "07f7c6b6ffe6: Waiting\n",
      "8adfa74289f9: Waiting\n",
      "28efa186d852: Waiting\n",
      "6276f4f9c29d: Verifying Checksum\n",
      "6276f4f9c29d: Download complete\n",
      "a4bd43ad48ce: Verifying Checksum\n",
      "a4bd43ad48ce: Download complete\n",
      "3c654ad3ed7d: Verifying Checksum\n",
      "3c654ad3ed7d: Download complete\n",
      "16c48d79e9cc: Verifying Checksum\n",
      "16c48d79e9cc: Download complete\n",
      "07f7c6b6ffe6: Verifying Checksum\n",
      "07f7c6b6ffe6: Download complete\n",
      "f190bad819d1: Verifying Checksum\n",
      "f190bad819d1: Download complete\n",
      "9bd8013aa86d: Verifying Checksum\n",
      "9bd8013aa86d: Download complete\n",
      "28efa186d852: Verifying Checksum\n",
      "28efa186d852: Download complete\n",
      "6865d2339eef: Verifying Checksum\n",
      "6865d2339eef: Download complete\n",
      "2a7bad8c8573: Verifying Checksum\n",
      "2a7bad8c8573: Download complete\n",
      "5fcc4cd817ce: Verifying Checksum\n",
      "5fcc4cd817ce: Download complete\n",
      "8adfa74289f9: Verifying Checksum\n",
      "8adfa74289f9: Download complete\n",
      "0776e061f5e5: Verifying Checksum\n",
      "0776e061f5e5: Download complete\n",
      "2ebe4d9e0c47: Verifying Checksum\n",
      "2ebe4d9e0c47: Download complete\n",
      "fd548bf41707: Verifying Checksum\n",
      "fd548bf41707: Download complete\n",
      "f7598aa00566: Verifying Checksum\n",
      "f7598aa00566: Download complete\n",
      "5db91f0f2b44: Verifying Checksum\n",
      "5db91f0f2b44: Download complete\n",
      "16c48d79e9cc: Pull complete\n",
      "3c654ad3ed7d: Pull complete\n",
      "6276f4f9c29d: Pull complete\n",
      "a4bd43ad48ce: Pull complete\n",
      "f190bad819d1: Pull complete\n",
      "07f7c6b6ffe6: Pull complete\n",
      "9bd8013aa86d: Pull complete\n",
      "28efa186d852: Pull complete\n",
      "8adfa74289f9: Pull complete\n",
      "6865d2339eef: Pull complete\n",
      "2a7bad8c8573: Pull complete\n",
      "5fcc4cd817ce: Pull complete\n",
      "2ebe4d9e0c47: Pull complete\n",
      "0776e061f5e5: Pull complete\n",
      "fd548bf41707: Pull complete\n",
      "5db91f0f2b44: Pull complete\n",
      "f7598aa00566: Pull complete\n",
      "Digest: sha256:936c33b65f45937341062e5d1a3a3fcff7270f59038ca119f022dc60da44a5a8\n",
      "Status: Downloaded newer image for azuremldemo2a8587f1.azurecr.io/azureml/azureml_ea4f16c22b1c1824315a89a744e988cb:latest\n",
      "97f401fb04275360b1d6423346ff88941fc2d740911e6b6675777a9dab68b781\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "bash: /azureml-envs/azureml_309e01c965cf2e25939ae807d69835c7/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting job preparation. Current time:2019-10-08T23:27:18.189602\n",
      "Extracting the control code.\n",
      "Creating directory: azureml-logs/\n",
      "Retrieving project from URI: https://azuremldemo7980688565.blob.core.windows.net/azureml-blobstore-12aea559-641a-4a9b-919d-04ce0cc6ddd6/azureml/project_zip_c994f3761d70407d9368f593992c4487?sv=2018-11-09&sr=b&sig=hDzS4gi9jme96GKjAZqe%2F8pIPoUOoCgqbVZJ8hHH2zk%3D&st=2019-10-08T23%3A11%3A56Z&se=2019-10-15T23%3A21%3A56Z&sp=r\n",
      "Download from datastores if requested.\n",
      "Download or mount from datasets if requested.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "bash: /azureml-envs/azureml_309e01c965cf2e25939ae807d69835c7/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "bash: /azureml-envs/azureml_309e01c965cf2e25939ae807d69835c7/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 137\n",
      "Entering Run History Context Manager.\n",
      "/azureml-envs/azureml_309e01c965cf2e25939ae807d69835c7/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/azureml-envs/azureml_309e01c965cf2e25939ae807d69835c7/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "Predicted value: ['male']\n",
      "Accuracy 1.0\n",
      "Export the model to model.pkl\n",
      "Import the model from model.pkl\n",
      "New Sample: [[154, 54, 35]]\n",
      "Predicted class: ['female']\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "bash: /azureml-envs/azureml_309e01c965cf2e25939ae807d69835c7/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting job release. Current time:2019-10-08T23:27:29.809310\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 162\n",
      "Job release is complete. Current time:2019-10-08T23:27:32.078133\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: framework_1570576914_938b648c\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/09f45657-2113-4b94-8f6f-cca0449e7fc3/resourceGroups/jcantrell-rg1/providers/Microsoft.MachineLearningServices/workspaces/azureml-demo/experiments/framework/runs/framework_1570576914_938b648c\n",
      "\n",
      "CPU times: user 5.1 s, sys: 286 ms, total: 5.38 s\n",
      "Wall time: 5min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'endTimeUtc': '2019-10-08T23:27:46.084669Z',\n",
       " 'inputDatasets': [],\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt': 'https://azuremldemo7980688565.blob.core.windows.net/azureml/ExperimentRun/dcid.framework_1570576914_938b648c/azureml-logs/55_azureml-execution-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt?sv=2018-11-09&sr=b&sig=4gQY7xvSQHQJI2pwprDfwwuDLxCoeCZfK6cIGBbGtqs%3D&st=2019-10-08T23%3A17%3A47Z&se=2019-10-09T07%3A27%3A47Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt': 'https://azuremldemo7980688565.blob.core.windows.net/azureml/ExperimentRun/dcid.framework_1570576914_938b648c/azureml-logs/65_job_prep-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt?sv=2018-11-09&sr=b&sig=35klDQzvm8FMMuXSIFXP91CgsQSyaWVpgjqJyodYNps%3D&st=2019-10-08T23%3A17%3A47Z&se=2019-10-09T07%3A27%3A47Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://azuremldemo7980688565.blob.core.windows.net/azureml/ExperimentRun/dcid.framework_1570576914_938b648c/azureml-logs/70_driver_log.txt?sv=2018-11-09&sr=b&sig=NDW%2BfNy8hdTsJOLnuqZrEiOV8ahl2q5YKlwZKh3pzdc%3D&st=2019-10-08T23%3A17%3A47Z&se=2019-10-09T07%3A27%3A47Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt': 'https://azuremldemo7980688565.blob.core.windows.net/azureml/ExperimentRun/dcid.framework_1570576914_938b648c/azureml-logs/75_job_post-tvmps_40c352e2384c559aac6cafd69b46c12ee22246ff0d14e4ea61f578b2b0de87e2_d.txt?sv=2018-11-09&sr=b&sig=fpWZJV7AdqnEq%2FfW63b%2FFwyloydbWXjwg%2FHxpo1BN2Q%3D&st=2019-10-08T23%3A17%3A47Z&se=2019-10-09T07%3A27%3A47Z&sp=r',\n",
       "  'logs/azureml/137_azureml.log': 'https://azuremldemo7980688565.blob.core.windows.net/azureml/ExperimentRun/dcid.framework_1570576914_938b648c/logs/azureml/137_azureml.log?sv=2018-11-09&sr=b&sig=uuRLeCQG%2Bwgv98XcKCYfm5%2FYCwIcX7DKmsmMcklq1Rc%3D&st=2019-10-08T23%3A17%3A47Z&se=2019-10-09T07%3A27%3A47Z&sp=r',\n",
       "  'logs/azureml/azureml.log': 'https://azuremldemo7980688565.blob.core.windows.net/azureml/ExperimentRun/dcid.framework_1570576914_938b648c/logs/azureml/azureml.log?sv=2018-11-09&sr=b&sig=u%2Bw7en3NkBIIgOBUbZdrw8Ut51q27G3EQbNYseU3c1g%3D&st=2019-10-08T23%3A17%3A47Z&se=2019-10-09T07%3A27%3A47Z&sp=r'},\n",
       " 'properties': {'AzureML.DerivedImageName': 'azureml/azureml_ea4f16c22b1c1824315a89a744e988cb',\n",
       "  'ContentSnapshotId': 'bbf6d0d6-f0d3-4c6d-adb1-9e9baafbc9a5',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json',\n",
       "  '_azureml.ComputeTargetType': 'batchai'},\n",
       " 'runDefinition': {'amlCompute': {'clusterMaxNodeCount': None,\n",
       "   'name': None,\n",
       "   'retainCluster': False,\n",
       "   'vmSize': None},\n",
       "  'arguments': [],\n",
       "  'communicator': 'None',\n",
       "  'containerInstance': {'cpuCores': 2, 'memoryGb': 3.5, 'region': None},\n",
       "  'data': {},\n",
       "  'dataReferences': {},\n",
       "  'docker': {'arguments': [],\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'useDocker': True},\n",
       "  'environment': {'docker': {'arguments': [],\n",
       "    'baseDockerfile': None,\n",
       "    'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'baseImageRegistry': {'address': None, 'password': None, 'username': None},\n",
       "    'enabled': True},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'inferencingStackVersion': None,\n",
       "   'name': 'fwrk',\n",
       "   'python': {'baseCondaEnvironment': None,\n",
       "    'condaDependencies': {'channels': ['conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults==1.0.65.*',\n",
       "        'inference-schema[numpy-support]']},\n",
       "      'scikit-learn',\n",
       "      'pandas',\n",
       "      'numpy',\n",
       "      'seaborn',\n",
       "      'category_encoders',\n",
       "      'lightgbm',\n",
       "      'papermill'],\n",
       "     'name': 'azureml_309e01c965cf2e25939ae807d69835c7'},\n",
       "    'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False},\n",
       "   'spark': {'packages': [], 'precachePackages': True, 'repositories': []},\n",
       "   'version': 'Autosave_2019-10-08T11:03:07Z_df8bef26'},\n",
       "  'exposedPorts': None,\n",
       "  'framework': 'Python',\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'history': {'directoriesToWatch': ['logs'],\n",
       "   'outputCollection': True,\n",
       "   'snapshotProject': True},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'nodeCount': 1,\n",
       "  'script': 'train.py',\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'target': 'AzureMLFramework',\n",
       "  'tensorflow': {'parameterServerCount': 1, 'workerCount': 1}},\n",
       " 'runId': 'framework_1570576914_938b648c',\n",
       " 'startTimeUtc': '2019-10-08T23:24:48.972466Z',\n",
       " 'status': 'Completed',\n",
       " 'target': 'AzureMLFramework'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# specify show_output to True for a verbose log\n",
    "run.wait_for_completion(show_output=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the Model\n",
    "\n",
    "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwrk\tfwrk:6\t6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "os.stat_result(st_mode=33279, st_ino=14123310421666430976, st_dev=45, st_nlink=1, st_uid=0, st_gid=0, st_size=1889, st_atime=1570533517, st_mtime=1570577268, st_ctime=1570577268)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='fwrk', model_path='outputs/fwrk.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "import os \n",
    "ws = Workspace.from_config()\n",
    "model=Model(ws, 'fwrk')\n",
    "\n",
    "model.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "\n",
    "# verify the downloaded model file\n",
    "file_path = os.path.join(os.getcwd(), \"fwrk.pkl\")\n",
    "\n",
    "os.stat(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model as an AzureML Service\n",
    "\n",
    "#### Create the Scoring Script\n",
    "\n",
    "Create the scoring script, called score.py, used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
    "\n",
    "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    #model = joblib.load('recommender.pkl')\n",
    "    model_path = Model.get_model_path('fwrk')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "input_sample = pd.DataFrame(data=[{\n",
    "              \"input_name_1\": 5,         # This is a decimal type sample. Use the data type that reflects this column in your data\n",
    "              \"input_name_2\": 5,    # This is a string type sample. Use the data type that reflects this column in your data\n",
    "              \"input_name_3\": 3            # This is a integer type sample. Use the data type that reflects this column in your data\n",
    "            }])\n",
    "\n",
    "output_sample = np.array([0])              # This is a integer type sample. Use the data type that reflects the expected result\n",
    "\n",
    "@input_schema('data', PandasParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        # you can return any datatype as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy in ACI\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "1. Build an image using:\n",
    "   * The scoring file\n",
    "   * The environment file\n",
    "   * The model file\n",
    "1. Register that image under the workspace. \n",
    "1. Send the image to the ACI container.\n",
    "1. Start up a container in ACI using the image.\n",
    "1. Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"SVM\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Predict gender with sklearn SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Running..........................................................\n",
      "Succeeded\n",
      "Image creation operation finished for image fwrk:4, operation \"Succeeded\"\n",
      "Running.................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n",
      "CPU times: user 680 ms, sys: 59.2 ms, total: 740 ms\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "# configure the image\n",
    "image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n",
    "                                                  runtime=\"python\", \n",
    "                                                  conda_file=\"fwrk.yml\")\n",
    "\n",
    "service_name = 'fwrk'\n",
    "\n",
    "# delete service if it exists\n",
    "try:\n",
    "    service = Webservice(ws, name=service_name)\n",
    "    if service:\n",
    "        service.delete()\n",
    "except WebserviceException as e:\n",
    "    print()\n",
    "    \n",
    "service = Webservice.deploy_from_model(workspace=ws, \n",
    "                                       name=service_name, \n",
    "                                       deployment_config=aciconfig, \n",
    "                                       models=[model], \n",
    "                                       image_config=image_config)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://b657addc-ef8a-4710-b48f-87c64d8e2389.eastus2.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test deployed service\n",
    "\n",
    "Earlier you scored all the test data with the local version of the model. Now, you can test the deployed model.  \n",
    "\n",
    "The following code goes through these steps:\n",
    "1. Send the data as a JSON array to the web service hosted in ACI. \n",
    "\n",
    "1. Use the SDK's `run` API to invoke the service. You can also make raw calls using any HTTP tool such as curl.\n",
    "\n",
    "1. Print the returned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Content-Type': 'application/json'}\n",
      "200\n",
      "0:00:00.160056\n",
      "['male']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "if service.auth_enabled:\n",
    "    headers['Authorization'] = 'Bearer '+service.get_keys()[0]\n",
    "\n",
    "print(headers)\n",
    "    \n",
    "test_sample = json.dumps({'data': [[190, 70, 43]]})\n",
    "\n",
    "response = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.elapsed)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
